{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3O5_IDnOdr",
        "colab_type": "text"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MxLj0lent2P",
        "colab_type": "text"
      },
      "source": [
        "Mount drive to get data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDTrnGWuGfMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmS0FsFngBF",
        "colab_type": "text"
      },
      "source": [
        "**Download relevant repos:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SIE8-B1knd4y",
        "colab": {}
      },
      "source": [
        "!pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "import torch, torchvision\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import itertools\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import operator\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks, peak_widths\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog\n",
        "from detectron2.data.datasets import load_coco_json\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFHraPwpwLgO",
        "colab_type": "text"
      },
      "source": [
        "# **Image Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C62g7PX8jjl",
        "colab_type": "text"
      },
      "source": [
        "The image segmentation code is based on the tutorial which can be found here: \n",
        "https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWpiaXTon1tw",
        "colab_type": "text"
      },
      "source": [
        "Load anotations and datafiles, and register as COCO file format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg71LnaWCy1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DatasetCatalog.register(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/\",\n",
        "                        lambda: load_coco_json(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/trainval.json\", \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/\"))\n",
        "MetadataCatalog.get(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/trainval.json\").set(thing_classes=[\"femur\",\"patella\", \"tibia\"])\n",
        "\n",
        " metadata = MetadataCatalog.get(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/trainval.json\").set(thing_classes=[\"femur\",\"patella\", \"tibia\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFxiCoEs0DSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dicts  = load_coco_json(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/trainval.json\", \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/MOST/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkAMOeU1oM9d",
        "colab_type": "text"
      },
      "source": [
        "Setup model, with pretrained weights, and retrain the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.OUTPUT_DIR = \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/\"\n",
        "cfg.DATASETS.TRAIN = (\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025*10\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # only has one class (ballon)\n",
        "cfg.MODEL.DENSEPOSE_ON = True\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/\", \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set the testing threshold for this model\n",
        "cfg.DATASETS.TEST = (\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/MOST/\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHbrYKzzoYSh",
        "colab_type": "text"
      },
      "source": [
        "Dump model weights as a cfg file format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcSR67ItCqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml\n",
        "with open('cfg.yaml', 'w') as f:\n",
        "  yaml.dump(cfg, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0S1KsgJohRr",
        "colab_type": "text"
      },
      "source": [
        "Import model retrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OGRNfoGykt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2_repo.projects.DensePose.densepose.config import add_densepose_config\n",
        "\n",
        "cfg1 = get_cfg()\n",
        "add_densepose_config(cfg1)\n",
        "cfg1.merge_from_file(\"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/cfg.yaml\")\n",
        "cfg1.OUTPUT_DIR = \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/Validation/\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7_2WSdtonqI",
        "colab_type": "text"
      },
      "source": [
        "# Train the model using early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTndgwdosC0",
        "colab_type": "text"
      },
      "source": [
        "First define the loss function, this is done with DICE coefficients:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGHZp-24GNCd",
        "colab_type": "text"
      },
      "source": [
        "The DICE coefficient calculation are based on the code which can be found here: \n",
        "https://github.com/milesial/Pytorch-UNet/blob/master/dice_loss.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FTcr2m2VkRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "\n",
        "class DiceCoeff(Function):\n",
        "    \"\"\"Dice coeff for individual examples\"\"\"\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        self.save_for_backward(input, target)\n",
        "        eps = 0.0001\n",
        "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
        "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
        "\n",
        "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
        "        return t\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    def backward(self, grad_output):\n",
        "\n",
        "        input, target = self.saved_variables\n",
        "        grad_input = grad_target = None\n",
        "\n",
        "        if self.needs_input_grad[0]:\n",
        "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
        "                         / (self.union * self.union)\n",
        "        if self.needs_input_grad[1]:\n",
        "            grad_target = None\n",
        "\n",
        "        return grad_input, grad_target\n",
        "\n",
        "\n",
        "def dice_coeff(input, target):\n",
        "    \"\"\"Dice coeff for batches\"\"\"\n",
        "    if input.is_cuda:\n",
        "        s = torch.FloatTensor(1).cuda().zero_()\n",
        "    else:\n",
        "        s = torch.FloatTensor(1).zero_()\n",
        "\n",
        "    for i, c in enumerate(zip(input, target)):\n",
        "        s = s + DiceCoeff().forward(c[0], c[1])\n",
        "\n",
        "    return s / (i + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXKn5DBppBJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "from PIL import ImageDraw\n",
        "from torchvision import transforms\n",
        "import PIL.Image\n",
        "import PIL\n",
        "try:\n",
        "    import Image\n",
        "except ImportError:\n",
        "    from PIL import Image\n",
        "\n",
        "labels_true=[\"femur\",\"patella\", \"tibia\"]\n",
        "\n",
        "def polygons_to_mask(img_shape, polygons):\n",
        "    mask = np.zeros(img_shape, dtype=np.uint8)\n",
        "    mask = PIL.Image.fromarray(mask)\n",
        "    xy = list(map(tuple, polygons))\n",
        "    PIL.ImageDraw.Draw(mask).polygon(xy=xy, outline=1, fill=1)\n",
        "    mask = np.array(mask, dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def merge(list1, list2):   \n",
        "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))] \n",
        "    return merged_list \n",
        "\n",
        "def calculate_dice(dataset_dicts, predictor):\n",
        "  bone_acc=[]\n",
        "  for d in dataset_dicts[:]:    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    h=d['height']\n",
        "    w=d['width']\n",
        "    outputs = predictor(im)\n",
        "    dap=outputs['instances'].pred_masks.to('cpu')*1\n",
        "    doop=outputs['instances'].to('cpu')\n",
        "    label_nr=doop.pred_classes.numpy()\n",
        "    bop=d['annotations']\n",
        "    visualizer = Visualizer(im[:, :, ::-1], metadata= metadata, scale=0.5)\n",
        "    vis = visualizer.draw_instance_predictions(doop)    \n",
        "    class_names=metadata.get(\"thing_classes\", None)\n",
        "    classes=doop.pred_classes\n",
        "    labels = [class_names[i] for i in classes]\n",
        "\n",
        "    if len(labels) < 2.5: \n",
        "      return 0\n",
        "    label_nr=0\n",
        "    \n",
        "    for i in labels:\n",
        "      predict_bone=dap[label_nr]\n",
        "      label_nr+=1\n",
        "      for y in bop:\n",
        "        if labels_true[y['category_id']]==i:\n",
        "          \n",
        "          points=y['segmentation']\n",
        "          points_np=points[0]\n",
        "          yval=points_np[1::2]\n",
        "          xval=points_np[0::2]\n",
        "          points_m=merge(xval,yval)\n",
        "          true_bone=polygons_to_mask([h,w],points_m)\n",
        "          true_bone=true_bone*1\n",
        "          trans=transforms.ToTensor()\n",
        "          true_bony=trans(true_bone)\n",
        "          predict_bony = predict_bone.unsqueeze(0)\n",
        "          \n",
        "          predict_bony = predict_bony.unsqueeze(0)\n",
        "          true_bony = true_bony.unsqueeze(0)\n",
        "          \n",
        "          tot = dice_coeff(predict_bony, true_bony)\n",
        "          dum_dum=true_bone-(predict_bone.numpy()*255)\n",
        "          sum_dum=sum(sum(dum_dum))\n",
        "          bone_acc.append(tot)\n",
        "          if i=='femur':\n",
        "            femur_acc=tot\n",
        "          if i=='patella':\n",
        "            patella_acc=tot\n",
        "          if i=='tibia':\n",
        "            tibia_acc=tot\n",
        "          \n",
        "          if sum_dum < 0: \n",
        "            sum_dum=sum_dum*-1\n",
        "          resul=(sum(sum(true_bone))-sum_dum)/sum(sum(true_bone))\n",
        "  return np.mean(bone_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l78P-RDqUoL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_error = calculate_dice(validation_dataset, predictor)\n",
        "iter_nr=100\n",
        "error_yuppi=[]\n",
        "stopper = 0\n",
        "\n",
        "while stopper < 300:\n",
        "  stopper += 10\n",
        "  iter_nr+=10\n",
        "  cfg.SOLVER.MAX_ITER = iter_nr\n",
        "  cfg.DATASETS.TEST = (\"/content/drive/My Drive/LAT_IMGS/Validation/\",)   # \n",
        "  trainer = DefaultTrainer(cfg)\n",
        "  trainer.resume_or_load(resume=True)\n",
        "  trainer.train()\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6  \n",
        "\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "  tmp = calculate_dice(validation_dataset, predictor)\n",
        "  error_yuppi.append(tmp)\n",
        "  if (tmp>val_error):\n",
        "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "    val_error = tmp\n",
        "    stopper = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rggMKxAtpwtO",
        "colab_type": "text"
      },
      "source": [
        "# **Detection region of interest (ROI)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6_WuKVjw1Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "add_densepose_config(cfg)\n",
        "cfg.merge_from_file(\"/content/drive/My Drive/cfg_early_stop.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model_final_early_stop.pth\" \n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLWeLCdsOKX",
        "colab_type": "text"
      },
      "source": [
        "**Detection of images containing two knees.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTanT81CrqJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_dir = \"/content/drive/My Drive/LAT_IMGS/CNN model/lat/full/\"\n",
        "path = '/content/drive/My Drive/LAT_IMGS/CNN model/lat/two_knee_crop'\n",
        "items = os.listdir(\"/content/drive/My Drive/LAT_IMGS/CNN model/lat/full/\")\n",
        "\n",
        "for d in items:\n",
        "  im = cv2.imread(os.path.join(path_dir,d))\n",
        "  outputs = predictor(im)\n",
        "  dap = outputs['instances'].pred_masks.to('cpu')*1\n",
        "  dap = dap.permute(1,2,0)\n",
        "\n",
        "\n",
        "  # Check if more than 3 masks in image\n",
        "  if len(dap[0,0,:]) > 3:\n",
        "    tmp = 0\n",
        "\n",
        "    for i in range(len(dap[0,0,:])):\n",
        "      tmp += torch.sum(dap[:,:,i], 0)\n",
        "\n",
        "    tmp = tmp.data.numpy()\n",
        "    \n",
        "    peaks = find_peaks(tmp, height = (50,), distance = 500)\n",
        "    peak_min_idx = np.argmin(peaks[1]['peak_heights'])\n",
        "    peak_max_idx = np.argmax(peaks[1]['peak_heights'])\n",
        "    tmp_min_idx = peaks[0][peak_min_idx]\n",
        "\n",
        "    peak_min_width = np.uint16(np.floor(peak_widths(tmp, peaks= peaks[0])[0][peak_min_idx]))\n",
        "\n",
        "    if peak_min_idx < peak_max_idx:\n",
        "      im_copy = im[:,tmp_min_idx + peak_min_width:].copy()\n",
        "\n",
        "    elif peak_min_idx > peak_max_idx:\n",
        "      im_copy = im[:,:tmp_min_idx - peak_min_width].copy()\n",
        "\n",
        "    else:\n",
        "      print('Error')\n",
        "      print(d)\n",
        "\n",
        "    fullpath = os.path.join(path,d)\n",
        "    cv2.imwrite(fullpath,im_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixrtuMIVsuqR",
        "colab_type": "text"
      },
      "source": [
        "**ROI Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD8avovdssB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = os.listdir(\"/content/drive/My Drive/LAT_IMGS/CNN model/lat/full\")\n",
        "path_dir = \"/content/drive/My Drive/LAT_IMGS/CNN model/lat/full\"\n",
        "\n",
        "p = 0\n",
        "\n",
        "for d in items:\n",
        "    p += 1\n",
        "    if np.mod(p, 100) == 0:\n",
        "      print(d, p)\n",
        "    im = cv2.imread(os.path.join(path_dir,d))\n",
        "    outputs = predictor(im)\n",
        "\n",
        "    dap=outputs['instances'].pred_masks.to('cpu')*1\n",
        "\n",
        "    tmp = dap.permute(1,2,0)\n",
        "    tmp = tmp.data.numpy()\n",
        "    tmp = np.uint8(tmp*255)\n",
        "    img = np.zeros(shape = (tmp.shape[0],tmp.shape[1], 1), dtype = np.uint8)\n",
        "\n",
        "    #sums all the channels to 1 channel\n",
        "    for i in range(0,dap.shape[2]):\n",
        "      img[:,:,0] += dap[:,:,i]\n",
        "\n",
        "    _, thresh = cv2.threshold(img,1,255, cv2.THRESH_BINARY)\n",
        "\n",
        "    cnts = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "    \n",
        "    j=1\n",
        "    # Chekcs the number of contours found\n",
        "    # If only one or zero contours are present, erode the image untill atleast \n",
        "    # 2 contours is found\n",
        "    if len(cnts) < 2:\n",
        "      \n",
        "      kernel = np.ones((5,5),np.uint8)\n",
        "      while len(cnts) < 2:\n",
        "        erosion = cv2.erode(img,kernel,iterations = 1)\n",
        "        img = erosion\n",
        "        _, thresh = cv2.threshold(img,1,255, cv2.THRESH_BINARY)\n",
        "        cnts = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[1]\n",
        "        j += 1\n",
        "\n",
        "        if j > 10000:\n",
        "          boolean_factor = False\n",
        "          break\n",
        "\n",
        "      dilate = cv2.dilate(img,kernel,iterations = j-1)\n",
        "      img = dilate\n",
        "\n",
        "    # Compute the rectangular area of the found contours\n",
        "    cnts_area = list()\n",
        "    for i in range(0,len(cnts)):\n",
        "      cnts_area.append(cv2.contourArea(cnts[i]))\n",
        "\n",
        "    # Removes the smallest found countour areas untill only the \n",
        "    # Untill only the 2 largest areas remain    \n",
        "    while len(cnts)!=2:\n",
        "      cnts.pop(np.argmin(cnts_area))\n",
        "      cnts_area.pop(np.argmin(cnts_area))\n",
        "    \n",
        "    extTop, extBot = list(), list()\n",
        "    \n",
        "    #Compute the extrema of the found contours\n",
        "    for i in cnts:\n",
        "      c = i\n",
        "      extTop.append(tuple(c[c[:, :, 1].argmin()][0]))\n",
        "      extBot.append(tuple(c[c[:, :, 1].argmax()][0]))\n",
        "\n",
        "    tmp1 = (max(extTop, key = operator.itemgetter(1)))\n",
        "    tmp2 = (min(extBot, key = operator.itemgetter(1)))\n",
        "\n",
        "    # Compute the center of the ROI\n",
        "    cXmean = np.uint32(np.floor(np.mean(np.array([tmp1[0],tmp2[0]]))))\n",
        "    cYmean = np.uint32(np.floor(np.mean(np.array([tmp1[1],tmp2[1]]))))\n",
        "\n",
        "    ROI = im[np.int16(abs(np.floor(cYmean-im.shape[0]*0.2))):np.int16(abs(np.floor(cYmean+im.shape[0]*0.2))),\n",
        "              np.int16(abs(np.floor(cXmean-im.shape[1]*0.3))):np.int16(abs(np.floor(cXmean+im.shape[1]*0.3)))].copy()\n",
        "\n",
        "    path = \"/content/drive/My Drive/LAT_IMGS/CNN model/lat/roi/\"\n",
        "    fullpath = os.path.join(path,d)\n",
        "    cv2.imwrite(fullpath,ROI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NdaD4nNqX7h",
        "colab_type": "text"
      },
      "source": [
        "# **Train model on PA and lateral view separately**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhBVZHhS9QyH",
        "colab_type": "text"
      },
      "source": [
        "The code for training the PA and lateral view separately using Squeezenet is based on code which can be found here: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woDvNvFHqe9E",
        "colab_type": "text"
      },
      "source": [
        "Import Imbalanced dataset sampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KhH0JkIqAhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sampler import ImbalancedDatasetSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFTX7spNqhVc",
        "colab_type": "text"
      },
      "source": [
        "Import libraries and setup training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQsGdW25R2IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "data_dir = \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/CNN model/pa/roi\"\n",
        "model_name = \"squeezenet\"\n",
        "num_classes = 5\n",
        "batch_size = 16\n",
        "num_epochs = 200\n",
        "input_size = 224\n",
        "feature_extract = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0GupQUsqlQT",
        "colab_type": "text"
      },
      "source": [
        "Define one hot encoder:\n",
        "\n",
        "The one hot encoder are based on the code from the course material. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ1fi2wOPLA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(idx, batch_size):\n",
        "    \"\"\"\n",
        "    \n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    \"\"\"\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((len(idx),5))\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    for i in range(len(idx)):\n",
        "      one_hot[i,idx[i]] = 1.0\n",
        "    return one_hot\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D51kDN5CqqKq",
        "colab_type": "text"
      },
      "source": [
        "Define training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtEo7Jcg_sex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import*\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, lr_scheduler, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
        "                labels = torch.from_numpy(one_hot_encode(labels, batch_size)).type(torch.LongTensor).to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "               \n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    \n",
        "                    loss = criterion(outputs, torch.max(labels,1)[1])\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                labels_data = torch.max(labels.clone().detach(),1)[1]\n",
        "                preds_data = preds.clone().detach()\n",
        "                preds_data = preds_data.cpu()\n",
        "                labels_data = labels_data.cpu()\n",
        "                running_corrects += f1_score(labels_data, preds_data, average = 'weighted')\n",
        "                  \n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects*batch_size / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} F1-score: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            if phase == 'train':\n",
        "                scheduler.step(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val F1-score: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcLI8q_BA4Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL52f-pVqxUl",
        "colab_type": "text"
      },
      "source": [
        "Setup and load squeezenet model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B154VG3ITYqd",
        "colab_type": "code",
        "outputId": "9136c83a-9a7a-4edf-ed64-97b22e8dc53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\" to /root/.cache/torch/checkpoints/squeezenet1_0-a815701f.pth\n",
            "100%|██████████| 4.79M/4.79M [00:00<00:00, 117MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mDuY4rIq2Ok",
        "colab_type": "text"
      },
      "source": [
        "Create dataloader and dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPCr7r3a6WKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations import *\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, num_workers=4,\n",
        "                                                   sampler = ImbalancedDatasetSampler(image_datasets[x])) for x in ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Ay89q-q_97",
        "colab_type": "text"
      },
      "source": [
        "Define parameters to update, and define optimizer and learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6nB2chYUmJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = model_ft.to(device)\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode = 'min', patience=30, threshold = 0.001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLkqJMbUrIGq",
        "colab_type": "text"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT30WqlUUpcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNI2BYRrLoI",
        "colab_type": "text"
      },
      "source": [
        "# **Train model and both PA and lateral view**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQXFBGdY-Dog",
        "colab_type": "text"
      },
      "source": [
        "The code for training the PA and lateral view simultaneously using Squeezenet is based on code which can be found here: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4dYL7hpaGzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "data_dir_pa = \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/CNN model/pa/roi\"\n",
        "data_dir_lat = \"/content/drive/My Drive/Kvantitativ biologi og sygdomsmodellering/5. Semester/Deep learning/Project/LAT_IMGS/CNN model/lat/roi\"\n",
        "model_name = \"squeezenet\"\n",
        "num_classes = 5\n",
        "batch_size = 16\n",
        "num_epochs = 200\n",
        "input_size = 224\n",
        "feature_extract = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZPgNrj9aatp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import*\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    train_loss_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs_pa, inputs_lat, labels in dataloaders[phase]:\n",
        "                inputs_pa = inputs_pa.type(torch.FloatTensor).to(device)\n",
        "                inputs_lat = inputs_lat.type(torch.FloatTensor).to(device)\n",
        "                labels = torch.from_numpy(one_hot_encode(labels, batch_size)).type(torch.LongTensor).to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs_pa, inputs_lat)\n",
        "                    loss = criterion(outputs, torch.max(labels,1)[1])\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "          \n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs_pa.size(0)\n",
        "                labels_data = torch.max(labels.clone().detach(),1)[1]\n",
        "                preds_data = preds.clone().detach()\n",
        "                labels_data = labels_data.cpu()\n",
        "                preds_data = preds_data.cpu()\n",
        "                running_corrects += f1_score(labels_data, preds_data, average = 'weighted')\n",
        "                  \n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects*batch_size / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} F1-score: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                val_loss_history.append(epoch_loss)\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "                train_loss_history.append(epoch_loss)\n",
        "                scheduler.step(epoch_acc)\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val F1-score: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, val_loss_history, train_acc_history, train_loss_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDMHkc6HBvEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNfjWavErcKy",
        "colab_type": "text"
      },
      "source": [
        "Setup squeezenet and alter the model a bit to contain fully connected layers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BD0n04-LuS",
        "colab_type": "text"
      },
      "source": [
        "The setup of the squeezenet with fully connected layers added how to initialize the weights of the fully connected layeres are based on the code which can be found here: https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/models/squeezenet.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ0llBoFKRVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, absolute_import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.utils import model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['squeezenet1_0', 'squeezenet1_1', 'squeezenet1_0_fc512']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0':\n",
        "    'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1':\n",
        "    'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes\n",
        "    ):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(\n",
        "            squeeze_planes, expand1x1_planes, kernel_size=1\n",
        "        )\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(\n",
        "            squeeze_planes, expand3x3_planes, kernel_size=3, padding=1\n",
        "        )\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat(\n",
        "            [\n",
        "                self.expand1x1_activation(self.expand1x1(x)),\n",
        "                self.expand3x3_activation(self.expand3x3(x))\n",
        "            ], 1\n",
        "        )\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "    \"\"\"SqueezeNet.\n",
        "    Reference:\n",
        "        Iandola et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
        "        and< 0.5 MB model size. arXiv:1602.07360.\n",
        "    Public keys:\n",
        "        - ``squeezenet1_0``: SqueezeNet (version=1.0).\n",
        "        - ``squeezenet1_1``: SqueezeNet (version=1.1).\n",
        "        - ``squeezenet1_0_fc512``: SqueezeNet (version=1.0) + FC.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        version=1.0,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        #self.loss = loss\n",
        "        self.feature_dim = 512\n",
        "\n",
        "        if version == 1.0:\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "       \n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = self._construct_fc_layer(fc_dims, 1024, dropout_p)\n",
        "        self.classifier = nn.Linear(self.feature_dim, num_classes)\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n",
        "        \"\"\"Constructs fully connected layer\n",
        "        Args:\n",
        "            fc_dims (list or tuple): dimensions of fc layers, if None, no fc layers are constructed\n",
        "            input_dim (int): input dimension\n",
        "            dropout_p (float): dropout probability, if None, dropout is unused\n",
        "        \"\"\"\n",
        "        if fc_dims is None:\n",
        "            self.feature_dim = input_dim\n",
        "            return None\n",
        "\n",
        "        assert isinstance(\n",
        "            fc_dims, (list, tuple)\n",
        "        ), 'fc_dims must be either list or tuple, but got {}'.format(\n",
        "            type(fc_dims)\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        for dim in fc_dims:\n",
        "            layers.append(nn.Linear(input_dim, dim))\n",
        "            layers.append(nn.BatchNorm1d(dim))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout_p is not None:\n",
        "                layers.append(nn.Dropout(p=dropout_p))\n",
        "            input_dim = dim\n",
        "\n",
        "        self.feature_dim = fc_dims[-1]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x1,x2):\n",
        "        f1 = self.features(x1)\n",
        "        v1 = self.global_avgpool(f1)\n",
        "        v1 = v1.view(v1.size(0), -1)\n",
        "        f2 = self.features(x2)\n",
        "        v2 = self.global_avgpool(f2)\n",
        "        v2 = v1.view(v2.size(0), -1)\n",
        "        v3 = torch.cat((v1,v2),dim=1)\n",
        "        v3 = self.fc(v3)\n",
        "        y3 = self.classifier(v3)\n",
        "\n",
        "        return y3\n",
        "        \n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "    \n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url, map_location=None)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "def squeezenet1_0(num_classes, pretrained=True, **kwargs):\n",
        "    model = SqueezeNet(\n",
        "        num_classes, version=1.0, fc_dims=None, dropout_p=None, **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['squeezenet1_0'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0_fc512(\n",
        "    num_classes, pretrained=True, **kwargs\n",
        "):\n",
        "    model = SqueezeNet(\n",
        "        num_classes,\n",
        "        version=1.0,\n",
        "        fc_dims=[512,256, 128],\n",
        "        dropout_p=0.5,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['squeezenet1_0'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPS5P_63ItQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = squeezenet1_0_fc512(5,pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji00kLfyrpSZ",
        "colab_type": "text"
      },
      "source": [
        "Define the ImageFolder function, and alter it to pull both the PA and lateral view image every time the dataloader calls it. The code below are a copy of the original ImageFolder function just with alterations to the getitem part of the function in order to pull both the PA and lateral view image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRYINW4Slmb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def default_loader(path):\n",
        "        from torchvision import get_image_backend\n",
        "        if get_image_backend() == 'accimage':\n",
        "            return accimage_loader(path)\n",
        "        else:\n",
        "            return pil_loader(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQXWLzdJsN88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets.vision import VisionDataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "def has_file_allowed_extension(filename, extensions):\n",
        "    return filename.lower().endswith(extensions)\n",
        "def is_image_file(filename):   \n",
        "    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n",
        "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n",
        "    images = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
        "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "    if extensions is not None:\n",
        "        def is_valid_file(x):\n",
        "            return has_file_allowed_extension(x, extensions)\n",
        "    for target in sorted(class_to_idx.keys()):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        for root, _, fnames in sorted(os.walk(d)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                if is_valid_file(path):\n",
        "                    item = (path, class_to_idx[target])\n",
        "                    images.append(item)\n",
        "    return images\n",
        "\n",
        "class DatasetFolder(VisionDataset):\n",
        "    def __init__(self, root, loader, extensions=None, transform=None,\n",
        "                 target_transform=None, is_valid_file=None):\n",
        "        super(DatasetFolder, self).__init__(root, transform=transform,\n",
        "                                            target_transform=target_transform)\n",
        "        classes, class_to_idx = self._find_classes(self.root)\n",
        "        samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
        "        if len(samples) == 0:\n",
        "            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n",
        "                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
        "        self.loader = loader\n",
        "        self.extensions = extensions\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.samples = samples\n",
        "        self.targets = [s[1] for s in samples]\n",
        "    def _find_classes(self, dir):\n",
        "        if sys.version_info >= (3, 5):\n",
        "            # Faster and available in Python 3.5 and above\n",
        "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "        else:\n",
        "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "        classes.sort()\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        #original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        path = self.imgs[index][0].split(\"/roi/\")[-1]\n",
        "        sample_lat = self.loader(os.path.join(data_dir_lat, path))\n",
        "        if self.transform is not None:\n",
        "            sample_lat = self.transform(sample_lat)\n",
        "        # make a new tuple that includes original and the pat \n",
        "        return sample, sample_lat, target\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "def accimage_loader(path):\n",
        "    import accimage\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except IOError:\n",
        "        # Potentially a decoding problem, fall back to PIL.Image\n",
        "        return pil_loader(path)\n",
        "def default_loader(path):\n",
        "    from torchvision import get_image_backend\n",
        "    if get_image_backend() == 'accimage':\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "class ImageFolder(DatasetFolder):\n",
        "    def __init__(self, root, transform=None, target_transform=None,\n",
        "                 loader=default_loader, is_valid_file=None):\n",
        "        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
        "                                          transform=transform,\n",
        "                                          target_transform=target_transform,\n",
        "                                          is_valid_file=is_valid_file)\n",
        "        self.imgs = self.samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmaOUCcnrw93",
        "colab_type": "text"
      },
      "source": [
        "Import the Imbalanced dataset sampler which are altered a bit to this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU2bf_4DI6sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sampler_v2 import ImbalancedDatasetSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwNILa08r2ei",
        "colab_type": "text"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds-0jTcza2H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations import *\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: ImageFolder(os.path.join(data_dir_pa, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, num_workers=4,\n",
        "                                                   sampler = ImbalancedDatasetSampler(image_datasets[x])) for x in ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzcXuYhLB698",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import lr_scheduler\n",
        "model_ft =model.to(device)\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode = 'min', patience=30, threshold = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-ZUeVGHv720",
        "colab": {}
      },
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "# Train and evaluate\n",
        "model_ft, val_acc_hist, val_loss_hist, train_acc_hist, train_loss_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}